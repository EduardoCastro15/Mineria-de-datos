# -*- coding: utf-8 -*-
"""6 - AnalisisComponentesPrincipales.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pYkD7qXFkUW6f9CqpP2U-hjNdtMCebDb

***Carga de bibliotecas***
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import numpy as np
import pandas as pd
import seaborn as sns
import statsmodels.api as sm
import matplotlib.pyplot as plt
from statsmodels.multivariate.pca import PCA
from statsmodels.multivariate.factor import Factor
# %matplotlib inline

"""***Carga del dataset***"""

data = pd.read_csv('/content/drive/MyDrive/ESCOM/8vo Semestre/Data Mining/Extraordinario/Longley.csv')
data.head(5)

data.dtypes

"""***Análisis de componenetes principales***

El análisis de componentes principales son métodos de reducción de datos que se utilizan para volver a expresar datos multivariados con menos dimensiones. El objetivo de este método es reorientar los datos para que una multitud de variables originales se puedan resumir con relativamente pocos factores o componentes que capturen la máxima información posible de las variables originales.

El objetivo de la PCA es encontrar el componente z = [z1, z2, z3 ......] que son combinaciones lineales de las variables originales xy una cantidad para la máxima varianza posible.

DESCRIPCIÓN Un conjunto de datos macroeconómicos que proporciona un ejemplo bien conocido de regresión altamente colineal. Este marco de datos consta de 6 variables económicas, observadas anualmente desde 1947-62 

1) defaltor del PNB defaltor de precios implícito del PNB (producto nacional bruto) 

2) desempleados no. de desempleados 

3) Fuerzas Armadas no. de personas en las fuerzas armadas 

4) población población 'no institucionalizada'> = 14 años de edad 

5) empleada número de personas empleadas. 

6) Producto Nacional Bruto
"""

df = data.drop('Employed',axis=1)
df.head()

"""Dado que, haremos PCA en los datos para reducir las dimensiones, sigamos adelante y eliminemos la variable objetivo "employed".

***Visualiación de la tabla de correlación***
"""

correlation = df.corr()
correlation

"""***Visualiación de las tablas de calor***"""

plt.figure(figsize=(36,6), dpi=140)
for j,i in enumerate(['pearson','kendall','spearman']):
  plt.subplot(1,3,j+1)
  correlation = df.dropna().corr(method=i)
  sns.heatmap(correlation, linewidth = 2)
  plt.title(i, fontsize=18)

"""***Visualiación de la tabla modificada***"""

X = df
X.head()

"""***Visualiación de la variable objetivo***"""

Y = data['Employed']
Y.head()

X = sm.add_constant(X)

"""Esto agrega el término constante beta0 a la regresión lineal múltiple"""

model =  sm.OLS(Y,X).fit()

"""***Visualiación del resumen del modelo***"""

model.summary()

"""Notas: 

[1] Los errores estándar suponen que la matriz de covarianza de los errores está correctamente especificada. 

[2] El número de condición es grande, 1.74e + 05. Esto podría indicar que hay fuerte multicolinealidad u otros problemas numéricos.

Aquí podemos ver que tenemos una situación de "R ^ 2 alta pero pocas razones t significativas, lo que claramente nos dice que tenemos una situación de multicolinealidad y esto está violando nuestros supuestos de" No multicolinealidad "de regresión MCO.
"""

model = Factor(df).fit()
model.plot_scree()
plt.show()

pc = PCA(df,
         ncomp=2,
         standardize=True,
         demean=True,
         normalize=False,
         gls=False,
         weights=None,
         missing=None)

df_comp = pc.loadings.T
df_comp

X_factors = pc.factors
X_factors

correlation = X_factors.corr()
correlation

sns.heatmap(X_factors.corr())
plt.show()

"""Podemos ver que al utilizar la técnica PCA hemos eliminado por completo el problema de la multicolinealidad."""

X_pca=sm.add_constant(X_factors)

"""Esto agrega la constante beta0 a la regresión lineal múltiple."""

Y.head()

model = sm.OLS(Y,X_pca).fit()
model.summary()

"""Notas: 

[1] Los errores estándar suponen que la matriz de covarianza de los errores está correctamente especificada.
"""